{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "     <img src=\"https://raw.githubusercontent.com/hhelmbre/Rockstar-Lifestyle/master/doc/Logo.png\" width=\"19%\" align=\"left\">\n",
    " \n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "# Neural Network Using Python\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The folling jupyter notebook will use neural networks in order to get quantifiable data for given images. It uses the neuralnet.py python file and imcrop.py image manipulation python file\n",
    "\n",
    "---\n",
    "\n",
    "The following functions will be used:\n",
    "\n",
    "* neuralnet( dataset = [], NN_settings = None, train = False, save_settings = True, print_acc = False)\n",
    "    * Description: This function is a wrapper function that will use a neural network in order to best estimate the protein count of the input images. The solver used in this neural network is L-BFGS which is a quasi-Newton method that is theoretically and experimentally verified to have a faster convergence and works well with low-dimensional input. L-BFGS does not work well in other settings because of its high memory requirement and lack of use of minibatches. There are other methods that can be tried (Adam, SGD BatchNorm , Adadelta, Adagrad), but L-BFGS was the best choice for this application and for the time constraint given in producing this package.\n",
    "    * Parameters: \n",
    "    \n",
    "        - dataset: dataset to use neural network on\n",
    "        - NN_settings: classifier data\n",
    "        - train: decide on whether to train or test\n",
    "        - save_settings: decide on whether to save classifier data to 'data' folder\n",
    "        - print_acc: passes bool to accuracy() to tell function to print accuracy\n",
    "    \n",
    "    * Output:\n",
    "        - count: The counts of the input dataset\n",
    "    \n",
    "    \n",
    "* accuracy( test_x, test_y, classifier, output = False)\n",
    "    * Description: This function checks the accuracy of the neural network by sending testing data through the classifier and outputing the accuracy\n",
    "    * Parameters: \n",
    "    \n",
    "        - test_x: testing data\n",
    "        - test_y: answered data\n",
    "        - classifier: classifier object set by MLPClassifier\n",
    "        - output: boolean operator to determine whether to output info or not\n",
    "\n",
    "    * Output:\n",
    "        - acc: accuracy of the neural network\n",
    "    \n",
    "    \n",
    "* create_train_set(numb_sets=None, prev_set=None, bin_list=None, gauss_blur_list=None)\n",
    "    * Description: This function creates a training set of n datasets. If a previous set is provided, the function will add to the previous set and output a backup set along with the changed set.\n",
    "    \n",
    "     *Note: This function is weak, the image formation is not up to par and will be changed in the future to provide a better training set for the neural network. Right now, the training sets output only protein counts between 5600 and 5800 proteins which will not train the neural network properly.\n",
    "     \n",
    "    * Parameters: \n",
    "    \n",
    "        - numb_sets: number of training sets to create\n",
    "        - prev_set: list of training objs that usr might have\n",
    "        - bin_list: list of bins to use in MRH calculations\n",
    "        - gauss_blur_list: list of gauss blur vars to use\n",
    "    \n",
    "    * Output:\n",
    "        - prev_set: list of training data\n",
    "        - backup: list of backup training data in case new set is unacceptable\n",
    "\n",
    "* load_objects(name='untitled.dat')\n",
    "    * Description: This function will load an object-based dataset using dill which is a version of pickle (a function that stores large amounts of object data) into a subfolder\n",
    "    * Parameters: \n",
    "    \n",
    "        - name: name of filename to use, default = untitled.dat\n",
    "    \n",
    "    * Output:\n",
    "        - output: list of objects\n",
    "\n",
    "* _loadall(dir_par_path, name)\n",
    "    * Description: This function function is a private function for load_objects that will load a generator for the file name in the given path\n",
    "    * Parameters: \n",
    "    \n",
    "        - dir_par_path: folder path of the given file\n",
    "    \n",
    "    * Output:\n",
    "        - generator: generator of the loaded file\n",
    "          or\n",
    "          [] if directory doesn't exist\n",
    "\n",
    "* save_objects(dataset, name='untitled.dat')\n",
    "    * Description: This function will save an object set using dill which is a version of pickle (a function that stores large amounts of object data) into a subfolder named 'data'.\n",
    "    \n",
    "     *Note: If only saving a single object, put in a [list] of len 1 i.e. save_objects([obj], name = 'file_name')\n",
    "     \n",
    "    * Parameters: \n",
    "    \n",
    "        - dataset: array of objects to save\n",
    "        - name: name of file pickle will be saved to\n",
    "\n",
    "The following objects will be used:\n",
    "\n",
    "* TestObj()\n",
    "    * Description: Object for test and training values\n",
    "    * Contains:\n",
    "        - res : image resolution\n",
    "        - data : image data\n",
    "        - MRH : Multi Resolution Histogram data for image\n",
    "        - GB : Gaussian blur data for image\n",
    "        - bin_list : bin list for corresponding MRH\n",
    "        - heights : heights from MRH difference\n",
    "        - count : number of objects in image\n",
    "    \n",
    "* NNSettings()\n",
    "    * Description: Stores settings for neural network, mostly used for sgd; will be used more for future projects\n",
    "    * Contains:\n",
    "        - classifier : classifier object\n",
    "        - learn_rate : learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Before starting on the example, there is one thing to note: pickling is a dangerous method to use, the data used in this package is data which is simply compressed into ASCII text via pickling. This makes it so that anyone can adjust these parameters and access python os commands maliciously. For this demo,we will use pickling, but in the future we will look into other solutions for storing large sets of data.\n",
    "\n",
    "In addition to this, it is useful to note that these files are very large and people who wish to contribute or manipulate files will need to download and use [Git Large File Storage](https://git-lfs.github.com/). This is a pretty easy extension to use:\n",
    "1. Download the extension from the link provided\n",
    "2. Go to desired master branch\n",
    "3. Execute the following commands:\n",
    "\n",
    "    i. Set up git lfs\n",
    "    ```console\n",
    "    git lfs install \n",
    "    ```\n",
    "    ii. Select type of file to track\n",
    "    ```console\n",
    "    git lfs track \"*.dat\"\n",
    "    ```\n",
    "    iii. Make sure .gitattributes is tracked\n",
    "    ```console\n",
    "    git add .gitattributes \n",
    "    ```\n",
    "\n",
    "And thats it, git will automatically use lfs on all .dat files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Before going through this example, please use GLFS to download all files located in the /data/ folder of this repository into where your package is installed. For example, mine is located in:**\n",
    "\n",
    "**C:/Users/David/Miniconda3/lib/site-packages/rockstarlifestyle/data/**\n",
    "\n",
    "Future iterations wont need to do this, but the data files to run the neural network are large and take a very long time to produce. This example shows how to make a whole new training set from scratch, but it's not recommended right now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The packages we will be using in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from rockstarlifestyle import neuralnet as nn\n",
    "from rockstarlifestyle import imcrop as iC\n",
    "import skimage.io as sio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we need data for our NN, so we will run through the imcrop example which can be found in imcrop_demo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjust these depending on your img location + name:\n",
    "file = 'P10_PAM_ipsi_40x_hippo_scan_MaxIP'  \n",
    "filetype = '.png' \n",
    "path = r\"../Images/\" \n",
    "res_x = 256\n",
    "res_y = 256\n",
    "im_path = path + file+ filetype\n",
    "timpng = sio.imread(im_path)\n",
    "img_stack = iC.img_crop(path, file, filetype, (res_x,res_y))\n",
    "obj_stack = iC.stackmrh(img_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we have a stack of objects with all the data we need. We should next save this data as a pickle. This function will also create a folder named /data/ in your packages folder if you dont already have it. The code will automatically rename your file if you name it improperly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\Miniconda3\\lib\\site-packages\\rockstarlifestyle\\neuralnet.py:380: UserWarning: File saved does not end in '.dat', will have problems reading. Automatically renaming to 'stacked_img_test.dat'.\n",
      "  \"renaming to '{}'.\".format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to: C:/Users/David/Miniconda3/lib/site-packages/rockstarlifestyle/data/\n"
     ]
    }
   ],
   "source": [
    "nn.save_objects(obj_stack, name = 'stacked_img_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing to see if we saved correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = nn.load_objects(name = 'stacked_img_test.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rockstarlifestyle.imcrop.ImgID at 0x191c2849cc0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objects[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that we know how to save, lets load some files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = nn.load_objects('training_set.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.test_obj object at 0x00000191CD4B6B38>\n",
      "203\n",
      "[[  0.   0.   0. ...   0.   0.   0.]\n",
      " [  0.   0.   0. ...   0.   0.   0.]\n",
      " [  0.   0.   0. ...   0.   0.   0.]\n",
      " ...\n",
      " [  0.   0.   0. ...   0.   0.   0.]\n",
      " [  0.   0.   0. ...   0.   0.   0.]\n",
      " [  0.   0.   0. ... 255.   0. 255.]]\n"
     ]
    }
   ],
   "source": [
    "print(training_set[0])\n",
    "print(len(training_set))\n",
    "print(training_set[0].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training set is already pretty large, but lets add one more dataset to our training dataset as an example. We will create a new training set and also a backup in case our training set doesn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, backup = nn.create_train_set(numb_sets=1, prev_set=training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wowee, that sure did take forever! Good thing we already have a set already saved. Making 200 of these would take forever. Future work on this project will help to alleviate this wait time and also get better data for better training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.test_obj object at 0x00000191CD4B6B38>\n",
      "204\n",
      "[[  0.   0.   0. ...   0.   0.   0.]\n",
      " [  0.   0.   0. ...   0.   0.   0.]\n",
      " [  0.   0.   0. ...   0.   0.   0.]\n",
      " ...\n",
      " [  0.   0.   0. ...   0.   0.   0.]\n",
      " [  0.   0.   0. ...   0.   0.   0.]\n",
      " [  0.   0.   0. ... 255.   0. 255.]]\n"
     ]
    }
   ],
   "source": [
    "print(training_set[0])\n",
    "print(len(training_set))\n",
    "print(training_set[0].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, we have 1 extra dataset in our training array. Lets go ahead and save this for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to: C:/Users/David/Miniconda3/lib/site-packages/rockstarlifestyle/data/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\Miniconda3\\lib\\site-packages\\rockstarlifestyle\\neuralnet.py:380: UserWarning: File saved does not end in '.dat', will have problems reading. Automatically renaming to 'training_set.dat'.\n",
      "  \"renaming to '{}'.\".format(name))\n"
     ]
    }
   ],
   "source": [
    "nn.save_objects(training_set, 'training_set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets separate our data into an 80/20 split for training/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = training_set[0:int((len(training_set)*.8))]\n",
    "testing = training_set[int((len(training_set)*.8)):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create our neural network. To do this, we'll use the neuralnet() function which can both train and test datasets depending on the given user input\n",
    "\n",
    "For training, this function will train a classifier network using lbfgs which is a quasi-newton method using a modified Hessian. Based on research, even though it takes a lot of memory to use, it's a good choice for low dimmensional data like ours and has a very fast convergence rate when fitting data. It is also MUCH easier to work with than the more popular sgd (stochastic gradient descent) method which requires the user to fine-tune for loss function convergence (which is great, but not for our application). Other methods include (among many) Conjugate Gradient (CG) and Adam (an adaptive learning rate optimization algorithm). Future work will look more into these different methods, but due to project time constraints, we are going with lbfgs. We've set save_settings == True which means a classifier object containing the neural network info will be saved into our data folder as 'classifier_info.dat' for future use and modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to: C:/Users/David/Miniconda3/lib/site-packages/rockstarlifestyle/data/\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "nn.neuralnet(training, train = True, save_settings= True, print_acc = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets load it up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nn.load_objects('classifier_info.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This outputs an MLPClassifier object as a list, lets make it a single object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='lbfgs', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can test it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 41 values, 41 were True with an accuracy of 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5707, 5661, 5641, 5704, 5706, 5675, 5667, 5672, 5656, 5667, 5663,\n",
       "       5685, 5671, 5685, 5663, 5701, 5721, 5705, 5701, 5689, 5685, 5672,\n",
       "       5625, 5661, 5709, 5681, 5666, 5661, 5721, 5684, 5689, 5711, 5685,\n",
       "       5688, 5632, 5668, 5713, 5687, 5648, 5663, 5661])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.neuralnet(testing, nn_settings=classifier[0], train = False, save_settings=False, print_acc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So it seems to work well for the generated data, but how well does it work for real data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5715, 5680, 5681, 5694, 5662, 5684, 5680, 5661, 5704, 5662, 5680,\n",
       "       5662, 5685, 5694, 5662, 5678, 5662, 5680, 5684, 5690, 5704, 5694,\n",
       "       5681, 5710, 5684, 5680, 5694, 5680, 5684, 5662, 5662, 5710, 5662,\n",
       "       5685, 5680, 5672, 5704, 5681, 5696, 5704, 5708, 5662, 5662, 5680,\n",
       "       5706, 5680, 5654, 5662, 5704, 5678, 5699, 5680, 5681, 5662, 5668,\n",
       "       5662, 5662, 5711, 5704, 5689, 5662, 5685, 5672, 5689, 5684, 5706,\n",
       "       5662, 5662, 5709, 5662, 5672, 5680, 5689, 5699, 5699, 5680, 5662,\n",
       "       5705, 5662, 5654, 5690, 5694, 5694, 5705, 5662, 5704, 5662, 5680,\n",
       "       5704, 5703, 5721, 5680, 5661, 5684, 5680, 5684, 5681, 5662, 5662,\n",
       "       5680, 5680, 5708, 5711, 5704, 5669, 5704, 5672, 5680, 5707, 5662,\n",
       "       5710, 5680, 5684, 5704, 5690, 5684, 5684, 5708, 5709, 5685, 5680,\n",
       "       5662, 5662, 5663, 5713, 5704, 5699, 5672, 5662, 5662, 5668, 5680,\n",
       "       5688, 5662, 5680, 5648, 5662, 5662, 5684, 5680, 5663, 5684, 5704,\n",
       "       5710, 5661, 5705, 5684, 5705, 5688, 5654, 5654, 5680, 5680, 5684,\n",
       "       5680, 5710, 5721, 5704, 5680, 5684, 5667, 5684, 5680, 5680, 5680,\n",
       "       5724, 5662, 5662, 5681, 5654, 5654, 5680, 5680, 5684, 5684, 5680,\n",
       "       5662, 5684, 5672, 5704, 5672, 5680, 5690, 5680, 5680, 5680, 5710,\n",
       "       5680, 5680, 5684, 5642, 5680, 5704, 5680, 5684, 5680, 5662, 5662,\n",
       "       5667, 5705, 5680, 5680, 5681, 5617, 5704, 5680, 5680, 5680, 5684,\n",
       "       5662, 5662, 5690, 5708, 5662, 5710, 5684, 5662, 5704, 5696, 5708,\n",
       "       5680, 5684, 5648, 5689, 5704, 5681, 5684, 5680, 5662, 5662, 5668,\n",
       "       5691, 5709, 5684, 5669, 5705, 5662, 5662, 5705, 5658, 5704, 5672,\n",
       "       5708, 5694, 5681, 5684, 5662, 5684, 5662, 5672, 5672, 5669, 5662,\n",
       "       5704, 5681, 5662, 5662, 5632, 5662, 5662, 5717, 5672, 5680, 5681,\n",
       "       5708, 5680, 5680, 5681, 5696, 5667, 5703, 5662, 5711, 5662, 5680,\n",
       "       5672, 5662, 5662, 5662, 5680])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = nn.neuralnet(obj_stack, nn_settings=classifier[0], train = False, save_settings=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that doesn't look good. Every image contains about the same amount of proteins? That's not right... So, this is less a problem with the neural network itself and more of a problem with how we trained the network. Obviously the trained neural network will output results similar to the training dataset, our training figures only had images with pixels between 5600 and 5800 pixels. In future work, we will need to make image data a lot more robust in order to produce better results. There are also other solutions which are detailed extensively in the .py file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That concludes the neural network demo, there still needs to be a lot of work to be done, but the neural network itself seems to work very well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
